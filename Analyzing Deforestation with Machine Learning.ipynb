{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem definition\n",
    " \n",
    "Every 6 seconds in 2019, we lost one entire football field of primary rainforest (Weisse & Goldman, 2020). The destruction of forests is one of the most pressing and irreversibly impactful issues of our time. It is intrinsically tied to climate change, home and habitat loss, and income inequality (FAO & UNEP, 2020). Deforestation releases nearly 10% of all human greenhouse gas emissions (Rainforest Alliance, 2018). Yet, we have already cut almost half of all trees available and doubled our rate of deforestation in the past 15 years (Nunez, 2019). Despite all of our technological advancements, trees remain one of the most effective solutions for removing CO2, a main culprit of climate change, from the atmosphere (Bastin, 2020). We need to protect the world’s largest terrestrial carbon sink now (WWF, n.d.).\n",
    "\n",
    "Uncovering the land uses replacing forests can help us tackle this issue. With knowledge of the direct drivers of deforestation, decision-makers and stakeholders can develop and implement more effective policies and take more targeted action to curb forest loss (Irvin et al., 2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution specification\n",
    "\n",
    "Manual tracking and on-the-ground reporting of direct drivers, whilst accurate, is not scalable given the widespreadness of deforestation. Analysis of satellite imagery through machine learning (ML) models can therefore help us monitor forest loss at scale. ML techniques like decision-tree modeling, however, are limited for this application; they depend on high-resolution data that is not widely available and do not make full use of the information (e.g. multiple land uses) present in satellite images (Irvin et al., 2020). Convolutional neural networks, a semantic segmentation approach, and transfer learning can fill in that gap. \n",
    "\n",
    "Framed under this deep learning approach, the problem we are seeking to solve now takes the shape of a classification task. The developed model will classify each image containing forest loss as one of the four land use categories: \"plantation,\" \"smallholder agriculture,\" \"grassland/shrubland,\" and \"other.\" Through this, we can identify the direct driver of deforestation in a region (Irvin et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convolutional neural network__\n",
    "\n",
    "Convolutional neural networks, often referred to as CNNs or ConvNets, are instances of deep learning and, of course, neural networks. This machine learning algorithm is often used for classifying images (Saha, 2018). As a neural network, it is modeled loosely on our own neural system (specifically, the visual cortex) and the concept of activation threshold. By assigning importance to aspects of the images (for example, pixels) through weights and biases that are learned, CNNs can differentiate images (Saha, 2018). \n",
    "\n",
    "The CNN used here has a Feature Pyramid Network and is build up from an EfficientNet-B2 backbone (Irvin et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Semantic segmentation__\n",
    "\n",
    "Semantic segmentation is a technique that seperates regions in images according to the object class they belong to (Papers With Code, n.d.). To do so, each pixel in the image is labelled according to the class or category it is predicted to represent. This is called pixel-level dense prediction (Papers With Code, n.d.; Jordan, 2018). \n",
    "\n",
    "Semantic segmentation is achieved through neural networks. State-of-the-art semantic segmentation is achieved through EfficientNets (see below for explanation) (Huynh & Boutry, 2020). \n",
    "\n",
    "For the problem at hand, pixel-level prediction allows us to differentiate with more granularility between the different and multiple land uses of deforested land (Irvin et al., 2020). When predicting, we use the mean of all the per-pixel predictions in an image to classify the image and forest loss region it represents as one type of deforestation driver (Irvin et al., 2020). This differs from the canonical approach of multi-class classification where the model does not seek to label every pixel in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transfer learning__\n",
    "\n",
    "By building on top of a transferrable (i.e. similar) model trained on other data (transfer learning), we save time and resources, which is useful when we lack (labelled) data and also environ- mentally friendly (Kostadinov, 2019).\n",
    "\n",
    "Here, we conduct transfer learning by using the pre-trained model EfficientNet-B2 as a backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EfficientNet-B2__\n",
    "\n",
    "EfficientNets are image classification models that achieve state-of-the art accuracy (as shown on ImageNet data). They are smaller and faster than the best existing CNNs (Tan & Le, 2020). EfficientNets are built up through an invertedd bottleneck MBConv, AutoML, and compound scaling (Tan & Le, 2020).  \n",
    "\n",
    "EfficientNet-B2 (an instance of EfficientNets) is used in this model as a backbone, meaning that it serves as a feature extracting network, allowing us to choose a layer for feature extraction (TenserFlow, 2020; StackOverflow, 2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Pyramid Network__\n",
    "\n",
    "The architecture of the final model is a Feature Pyramid Network. As the name implies, this CNN has a pyramid representation, and it uses a top-down pathway with lateral connections to detect objects at different scales (Lin et al., 2017). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This solution draws inspiration from Stanford ML Group's _ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery_. In this November 2020 paper, Irvin et al. (2020) present results from a state-of-the-art model they developed by comparing random forests (RF) and convulational neural networks (CNN) models, and by \"[investigating] the effect of (a) using scene data augmentation (SDA) where [they] randomly sample from the scenes and composite images during training to capture changes in landscape over time, (b) pre-training (PT) the model on a large land cover dataset in Indonesia that [they] curated, and (c) using multi-modal fusion with a variety of auxiliary predictors [(Aux)].\" For this project, I develop a CNN model and try replicating Irvin et al.'s (2020) results using their same dataset. The addition of SDA, PT, and Aux, whilst they do also contribute (though less significantly) to improving accuracy and f-scores, are beyond the scope of the current project. The largest improvements in their model performance metrics occur between the RF and CNN models and hence my focus on the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and analysis\n",
    "\n",
    "To effectively test the models, the data was randomly split into a training, validation, and testing set. To best replicate Irvin et al.'s (2020) results, I used the same segregation of the data into these three sets (downloadable from Stanford ML Group's website). \n",
    "\n",
    "The model performance is evaluated through the F1 score. The F1 score is a weighted harmonic mean of both the precision and recall metrics (Shung, 2018). Recall identifies, in our scenario, the fraction of images of the direct driver class that were actually identified ( = true positives / (true positives + false negatives)). Precision identifies, in our scenario, the fraction of images classified as a particular direct driver class that was actually correctly identified ( = true positives / (true positives + false positives)). This mix of the two scores provides an effective measure of the models because we wish to both:\n",
    "(1) make sure we find all the instances of the specific direct driver causing deforestation;\n",
    "(2) correctly attribute deforestation to the right direct driver. Optimizing on both of these measures through a weighted metric allows decision-makers, based on the model, to then define appropriate measures that include all forest loss areas driven by the direct driver category and differentiate between the specifically problematic direct drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "ForestNet, the CNN model with the same architecture and backbone as specified above but with the addition of scene data augmentation, pre-training, and multi-modal fusion with auxiliary variables, achieved \"high classification performance\" on the test set across the four direct driver classes (Irvin et al., 2020). Whilst the authors do not include specifications on each driver category for the pure CNN model, the overall (all categories combined) accuracy and F1 scores for all the models are described.  \n",
    "            \n",
    "The F1 scores obtained in Irvin et al's (2020) CNN model is of 0.75 on the validation set and of 0.70 on the test set. In the model developed here, the F1 score is _ on the validation set and _ on the test set. The difference in scores might be due to a difference in (a) reporting calculations, (b) the loss function specifications, and/or (c) hyperparameter tuning. Irvin et al. (2020) report using the average of 10 runs for validation results, and the best validation run for test results. What this means in practice is unclear and might differ from the implementation here. The current version of Irvin et al.'s (2020) paper does not go into enough detail on what their \"linear combination of segmentation and classification losses\" looks like, to effectively replicate it. Access to their code in the future will be useful to understand better what parameters, including the loss function, they used to build their model.\n",
    "\n",
    "\n",
    "In our model, the highest F1 score is obtained for the plantation driver category, like for Irvin et al.'s (2020) more complex model. This might be attributable to the fact that this driver class compromises about 40% of both the training data and the test data. The smallholder agriculture category holds the second highest F1 score, followed by the \"other\" category. Last is the grassland/shrubland category. This might be due to confusion with smallholder agriculture: the precision rate of smallholder agriculture is prioritized at the expense of the precision rate of the grassland/shrubland (see Table 2 in Irvin et al. (2020) for evidence of this). This tradeoff can make sense because it is valuable to identify all cases of agriculture fueling deforestation over logging and empty land that can possibly regrow naturally. If, however, we instead instead wanted to prioritize accurately recognizing _all_ the cases of shrubland or minimize false positives (or, generally, obtain a better F1 score) for the grassland/shrubland deforestation driver, we might want to adjust the training procedure. Specifically, when evaluating the model on the validation set after each epoch, we may wish to save the checkpoint with the highest F1 score for the specific category of concern (in this case, grassland/shrubland), instead of for the mean across all four categories.\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bastin, Jean-François. (2020, October). What if there were 1 trillion more trees? TED-Ed. https://ed.ted.com/lessons/can-we-build-a-perfect-forest-jean-francois-bastin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAO & UNEP. (2020). The State of the World’s Forests 2020: Forests, Biodiversity and People. www.fao.org/3/ca8642en/ca8642en.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huynh, L. D., & Boutry, N. (n.d.). A U-Net++ With Pre-Trained EfficientNet Backbone for Segmentation of Diseases and Artifacts in Endoscopy Images and Videos. Retrieved from http://ceur-ws.org/Vol-2595/endoCV2020_paper_id_11.pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irvin, J., Sheng, H., Ramachandran, N., Johnson-Yu, S., Zhou, S., Story, K., ... & Ng, A. Y. (2020). ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery. arXiv preprint arXiv:2011.05479.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irvin, J., Sheng, H., Ramachandran, N., Johnson-Yu, S., Zhou, S., Story, K., ... & Ng, A. Y. (2020). ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery [Slides]. Climate Change AI. Retrieved from https://www.climatechange.ai/papers/neurips2020/22.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jordan, J. (2018). An overiew of semantic image segmentation. Retrieved from https://www.jeremyjordan.me/semantic-segmentation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kostadinov, S. (2019). What is Deep Transfer Learning and Why Is It Becoming So Popular? Retrieved from https://towardsdatascience.com/what-is-deep-transfer-learning-and-why-is-it-becoming-so-popular-91acdcc2717a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lin, T-Y., Dollár, P. Girshick, R., He, K. Hariharan, B., & Belongie, S. (2017). Feature Pyramid Networks for Object Detection. Proceedings of the IEEE conference on computer vision and pattern recognition. Retrieved from https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nunez, C. (2019, February 7). Deforestation explained. National Geographic. www.nationalgeographic.com/environment/global-warming/deforestation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers With Code. (n.d.). Computer Vision: Semantic Segmentation. Retrieved from https://paperswithcode.com/task/semantic-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saha, S. (2018). A Comprehensive Guide to Convolutional Neural Networks - the ELI5 way. Towards Data Science. Retrieved from https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shung, P. (2018). Accuracy, Precision, Recall or F1? Towards Data Science. Retrieved from https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackOverflow. (2020). What means backbone in a neural network? Retrieved from https://stackoverflow.com/questions/59868132/what-means-backbone-in-a-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tan, M, & Le, Q. V. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenserflow. (2020). efficientnet/b2/classification. Retrieved from https://tfhub.dev/tensorflow/efficientnet/b2/classification/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weisse, M., & Goldman, E. D. (2020, June 2). We Lost a Football Pitch of Primary Rainforest Every 6 Seconds in 2019. World Resources Institute (WRI). www.wri.org/blog/2020/06/global-tree-cover-loss-data-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WWF. (n.d.). Forest and Climate. wwf.panda.org/our_work/our_focus/forests_practice/forest_climate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following GitHub repository holds the code and data used for this project: https://github.com/magalidebruyn/deforestation-machine-learning. Its contents are included and outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used is publicly available. It can be downloaded on Stanford ML Group's website: https://stanfordmlgroup.github.io/projects/forestnet/; or directly from here: http://download.cs.stanford.edu/deep/ForestNetDataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The dataset consists of 2,756 satellite images of forest loss events with driver annotations. Global Forest Change (GFC) published maps were used to obtain forest loss events, each represented as a polygon and associated with a year indicating when the forest loss event occurred. An expert interpreter annotated each event with the direct driver of deforestation using high resolution satellite imagery from Google Earth. The driver annotations were grouped into Plantation, Smallholder Agriculture, Grassland/shrubland, and Other.\n",
    "\n",
    "We captured each forest loss region with Landsat 8 satellite imagery acquired within five years of the event’s occurrence using a custom cloud-minimizing search procedure. Using this procedure, we obtained exactly one composite image for each example and additional images for any individual cloud-filtered scenes. Imagery was processed and downloaded using the Descartes Labs platform\" (Irvin et al., 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Irvin et al.'s code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reached out to Jeremy Irvin and Hao Sheng (the main authors) to inquire about the code behind their ForestNet paper, as I could not find it not publicly accessible. Their reply is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ForestNet email.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As their code is not yet published and publicly available, I worked on replicating their model given what they divulged in their Methods section (not very detailed) and publicly available repos, including the one mentioned above. Whilst the code in the repos provide some guidance, the repos do not implement the model Irvin et al. (2020) describe and so a significant amount of extrapolation and building up on my own was required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C: Source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data downloaded from http://download.cs.stanford.edu/deep/ForestNetDataset.zip \n",
    "\n",
    "df_train = pd.read_csv('./ForestNetDataset/test.csv')\n",
    "df_val = pd.read_csv('./ForestNetDataset/val.csv')\n",
    "df_test = pd.read_csv('./ForestNetDataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>merged_label</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>example_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small-scale agriculture</td>\n",
       "      <td>Smallholder agriculture</td>\n",
       "      <td>1.446543</td>\n",
       "      <td>100.799778</td>\n",
       "      <td>2007</td>\n",
       "      <td>examples/1.446543071130837_100.79977801432716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grassland shrubland</td>\n",
       "      <td>Grassland shrubland</td>\n",
       "      <td>-1.209593</td>\n",
       "      <td>122.040547</td>\n",
       "      <td>2016</td>\n",
       "      <td>examples/-1.2095927346679962_122.0405465386496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Timber plantation</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>-0.205606</td>\n",
       "      <td>102.938756</td>\n",
       "      <td>2014</td>\n",
       "      <td>examples/-0.20560633109394327_102.93875617664571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>-1.730708</td>\n",
       "      <td>113.711455</td>\n",
       "      <td>2012</td>\n",
       "      <td>examples/-1.7307080624887758_113.71145483864248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil palm plantation</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>1.429489</td>\n",
       "      <td>100.823635</td>\n",
       "      <td>2010</td>\n",
       "      <td>examples/1.429488982681029_100.82363463176235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label             merged_label  latitude   longitude  \\\n",
       "0  Small-scale agriculture  Smallholder agriculture  1.446543  100.799778   \n",
       "1      Grassland shrubland      Grassland shrubland -1.209593  122.040547   \n",
       "2        Timber plantation               Plantation -0.205606  102.938756   \n",
       "3                    Other                    Other -1.730708  113.711455   \n",
       "4      Oil palm plantation               Plantation  1.429489  100.823635   \n",
       "\n",
       "   year                                      example_path  \n",
       "0  2007     examples/1.446543071130837_100.79977801432716  \n",
       "1  2016    examples/-1.2095927346679962_122.0405465386496  \n",
       "2  2014  examples/-0.20560633109394327_102.93875617664571  \n",
       "3  2012   examples/-1.7307080624887758_113.71145483864248  \n",
       "4  2010     examples/1.429488982681029_100.82363463176235  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak at the training data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>merged_label</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>example_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>Secondary forest</td>\n",
       "      <td>Other</td>\n",
       "      <td>-4.546995</td>\n",
       "      <td>138.550939</td>\n",
       "      <td>2015</td>\n",
       "      <td>examples/-4.546995345493858_138.55093919394707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>Secondary forest</td>\n",
       "      <td>Other</td>\n",
       "      <td>-6.849402</td>\n",
       "      <td>139.293801</td>\n",
       "      <td>2016</td>\n",
       "      <td>examples/-6.849401821379706_139.29380101814974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>Timber plantation</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>-1.247071</td>\n",
       "      <td>116.438285</td>\n",
       "      <td>2012</td>\n",
       "      <td>examples/-1.2470707925742197_116.43828534266872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>Small-scale agriculture</td>\n",
       "      <td>Smallholder agriculture</td>\n",
       "      <td>-1.117901</td>\n",
       "      <td>120.538561</td>\n",
       "      <td>2004</td>\n",
       "      <td>examples/-1.117901069624485_120.53856116642237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>Secondary forest</td>\n",
       "      <td>Other</td>\n",
       "      <td>-0.214673</td>\n",
       "      <td>103.510579</td>\n",
       "      <td>2014</td>\n",
       "      <td>examples/-0.2146726494964884_103.51057948511793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label             merged_label  latitude   longitude  \\\n",
       "663         Secondary forest                    Other -4.546995  138.550939   \n",
       "664         Secondary forest                    Other -6.849402  139.293801   \n",
       "665        Timber plantation               Plantation -1.247071  116.438285   \n",
       "666  Small-scale agriculture  Smallholder agriculture -1.117901  120.538561   \n",
       "667         Secondary forest                    Other -0.214673  103.510579   \n",
       "\n",
       "     year                                     example_path  \n",
       "663  2015   examples/-4.546995345493858_138.55093919394707  \n",
       "664  2016   examples/-6.849401821379706_139.29380101814974  \n",
       "665  2012  examples/-1.2470707925742197_116.43828534266872  \n",
       "666  2004   examples/-1.117901069624485_120.53856116642237  \n",
       "667  2014  examples/-0.2146726494964884_103.51057948511793  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_data(df):\n",
    "    # Get all labels\n",
    "    y = df['merged_label']\n",
    "\n",
    "    # Get all images\n",
    "    x = []\n",
    "    for path in df_test['example_path']:\n",
    "        im = imageio.imread(path)\n",
    "        x.append(im)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'merged_label', 'latitude', 'longitude', 'year',\n",
       "       'example_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns are consistent\n",
    "df_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'merged_label', 'latitude', 'longitude', 'year',\n",
       "       'example_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total data points in datasets:\n",
      "train: 668\n",
      "validation 473\n",
      "test: 1616\n",
      "\n",
      "Percentage split of total dataset across datasets:\n",
      "train: 24.2 %\n",
      "validation: 17.2 %\n",
      "test: 58.6 %\n"
     ]
    }
   ],
   "source": [
    "# Check out sizes of dataset and split distribution\n",
    "num_train = len(df_train)\n",
    "num_val = len(df_val)\n",
    "num_test = len(df_test)\n",
    "\n",
    "print(\"\\nTotal data points in datasets:\")\n",
    "print(\"train:\", len(df_train))\n",
    "print(\"validation\", len(df_val))\n",
    "print(\"test:\", len(df_test))\n",
    "\n",
    "ttl = num_train + num_val + num_test\n",
    "print(\"\\nPercentage split of total dataset across datasets:\")\n",
    "print(\"train:\", round((num_train/ttl)*100, 1), \"%\")\n",
    "print(\"validation:\", round((num_val/ttl)*100, 1), \"%\")\n",
    "print(\"test:\", round((num_test/ttl)*100, 1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define directory paths for future reference\n",
    "DATA_DIR = './ForestNetDataset/'\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'testannot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data loader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific helper class for data extraction, \n",
    "# transformation, and preprocessing\n",
    "\n",
    "class Dataset(BaseDataset):\n",
    "    \"\"\"ForestNet Dataset. Fetch & read images and apply preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['plantation', 'agriculture', 'land', 'other']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        # extract certain classes from mask\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image\n",
    "# Choose image closest to year of forest loss\n",
    "# Check if meets quality requirements\n",
    "# Else choose composite image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data transformation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libs\n",
    "!pip install -U segmentation-models-pytorch albumentations --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_transf():\n",
    "    # Randomly crop images during training (160 x 160 pixels)\n",
    "    train_transform = [\n",
    "        albu.PadIfNeeded(min_height=160, min_width=160, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=160, width=160, always_apply=True),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_test_transf():\n",
    "    # Center crop images during prediction (160 x 160 pixels)\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(min_height=160, min_width=160, always_apply=True, border_mode=0),\n",
    "        albu.CenterCrop(height=160, width=160, always_apply=True),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a Landsat 8 satellite image centered around a region of forest loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code built up using PyTorch library package from https://github.com/qubvel/segmentation_models.pytorch \n",
    "# Code adapted from https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/utils/metrics.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "# import tenserflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "ENCODER = 'efficientnet-b2'\n",
    "# ENCODER_WEIGHTS = 'imagenet'\n",
    "ENCODER_WEIGHTS = None # random initialization of weights\n",
    "CLASSES = 4 # 4 direct drivers\n",
    "ACTIVATION = None # None for logits\n",
    "\n",
    "# Create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    # in_channels=3   # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "    classes=CLASSES, \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data the same way as during weights pretraining \n",
    "# because all models have pretrained encoders/weights\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets using model preprocessing\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_transf(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_test_transf(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders for feeding batches of data in neural network\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=12)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "# Dice Loss = F1 score\n",
    "# Linear combo of segmentation and classification losses - cross-entropy\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# Define performance metric: f-score \n",
    "metrics = [\n",
    "    smp.utils.metrics.FScore(), \n",
    "    # smp.utils.metrics.Accuracy(), \n",
    "    # smp.utils.metrics.Precision(), \n",
    "    # smp.utils.metrics.Recall(), \n",
    "]\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A note on Adam optimization:__ Adam  is a stochastic gradient descent optimization algorithm and useful for problems with demanding amounts of data as it is computationally efficient (Kingma & Ba, 2014). This fits our image-based dataset well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify from per-pixel scores/logits\n",
    "# Compute the per-pixel logit within the forest loss region/polygon\n",
    "# Compute mean of the per-pixel scores/logits in the polygon for each class\n",
    "# Assign highest mean score (amongst classes) to image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epoch runners \n",
    "# a simple loop iterating over dataloader's samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for specified number of epochs\n",
    "# During training, we evaluate the model on the validation set after each epoch \n",
    "# and save the checkpoint with the highest F1 score averaged over the four-driver categories.\n",
    "\n",
    "epoch_num = 10 \n",
    "\n",
    "max_score = 0\n",
    "\n",
    "for i in range(0, epoch_num):\n",
    "    # Evaluate model on the validation set after each log\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # Save checkpoint with the highest F1 score\n",
    "    # averaged over the four-driver categories\n",
    "    if max_score < valid_logs['f1_score']: \n",
    "        max_score = valid_logs['f1_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best saved checkpoint\n",
    "best_model = torch.load('./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test dataset\n",
    "# Like for training and validation set\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_test_transf(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D: Extra code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transfer learning using a EfficientNet-B2 backbone__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use EfficientNet-B2 for transfer learning \n",
    "# https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\n",
    "# https://www.tensorflow.org/hub/common_signatures/images#classification\n",
    "\n",
    "m = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b2/classification/1\")\n",
    "])\n",
    "m.build([None, expect_img_size, expect_img_size, 3])  # Batch input shape\n",
    "# ? What does the None stand for? Images? Why is it None?\n",
    "\n",
    "# m.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance of model\n",
    "# Metric chosen is f1 score \n",
    "# Outputed above through model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Irvin et al.'s CNN model performance:__\n",
    "\n",
    "Validation set accuracy: 0.80\n",
    "\n",
    "Validation set F1 score: 0.75\n",
    "\n",
    "\n",
    "\n",
    "Test set accuracy: 0.78\n",
    "\n",
    "Test set F1 score: 0.70"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
